{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/home/sachin/Documents/NIPR/Research/Data/AMPERE/processed/ampere_omni_2009_2021_10min.nc')\n",
    "ds['doy_sin'] = np.sin(2*np.pi*ds['doy']/365)\n",
    "ds['doy_cos'] = np.cos(2*np.pi*ds['doy']/365)\n",
    "#ds = ds.sel(dt=slice('2015-03-12', '2015-03-22'))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clock_angle(By, Bz):\n",
    "    angle = np.rad2deg(np.arctan2(By, Bz))\n",
    "    np.where(angle < 0, angle + 360, angle)\n",
    "    return angle\n",
    "\n",
    "def cross_polar_cap_pot(v_sw,By, Bz):\n",
    "    #Not in SI units\n",
    "    #from https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/96JA01742\n",
    "    #and  https://www.sws.bom.gov.au/Category/Solar/Solar%20Conditions/Solar%20Wind%20Clock%20Angle/Solar%20Wind%20Clock%20Angle.php\n",
    "\n",
    "    theta  = clock_angle(By, Bz)\n",
    "    B_L = np.sqrt(By**2 + Bz**2)\n",
    "\n",
    "    CPCP = 10e-4 * v_sw**2 + 11.7 * B_L * (np.sin(np.deg2rad(theta)/2))**3\n",
    "\n",
    "    return CPCP\n",
    "\n",
    "def calc_akasofu(v_sw, Bx, By, Bz):\n",
    "    #https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2002JA009283\n",
    "    #In SI units\n",
    "    v_sw = v_sw * 1e3\n",
    "    Bx = Bx * 10e-9\n",
    "    By = By * 10e-9\n",
    "    Bz = Bz * 10e-9\n",
    "    B = np.sqrt(Bx**2 + By**2 + Bz**2)\n",
    "    l_0 = 6371 * 7 * 1e3 #7 R_E\n",
    "    mu_0 = np.pi * 4 * 10e-7\n",
    "    theta  = clock_angle(By, Bz)\n",
    "    epsilon_A = (4 * np.pi / mu_0) * v_sw * B**2 * np.sin(theta / 2)**4 * l_0**2\n",
    "    return epsilon_A \n",
    "\n",
    "#ds['epislon_A'] = calc_akasofu(ds['flow_speed'], ds['BX_GSE'], ds['BY_GSE'], ds['BZ_GSE'])\n",
    "#ds['CPCP'] = cross_polar_cap_pot(ds['flow_speed'],  ds['BY_GSE'], ds['BZ_GSE'])\n",
    "#ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_count(ds, var):\n",
    "    #count number of NaNs in a data variable\n",
    "    con_data = ds[var].values\n",
    "    nan_count = np.count_nonzero(np.isnan(con_data))\n",
    "    nan_ratio = nan_count / con_data.size\n",
    "    not_nan = con_data.size - nan_count\n",
    "    return not_nan, nan_count, np.round(nan_ratio, 2)\n",
    "\n",
    "#nan_count(ds, 'jPar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers for the target variable and input variables\n",
    "input_scaler = StandardScaler()\n",
    "\n",
    "# Extract the target variable and reshape for scaling\n",
    "target_var = ds['jPar'].values  # shape (t (n), mlat (50), mlt (24))\n",
    "\n",
    "# Extract and scale input variables (variables that are dependent only on 'dt')\n",
    "input_vars = ['BX_GSE', 'BY_GSE', 'BZ_GSE', 'flow_speed', 'proton_density', 'F10.7', 'doy_sin','doy_cos']\n",
    "input_data = np.array([ds[var].values for var in input_vars]).T  # shape (22320, number_of_vars)\n",
    "input_data_scaled = input_scaler.fit_transform(input_data)\n",
    "\n",
    "file_path = f'scaler_id3.pkl'\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(input_scaler, file)\n",
    "\n",
    "\n",
    "def no_lookback(target_data, input_data):\n",
    "    X = input_data[:,np.newaxis,:]\n",
    "    y = target_data\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = no_lookback(target_var, input_data_scaled)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_time_series_split(X, y, train_ratio=0.9, test_ratio=0.05):\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_idx = int(len(X) * train_ratio)\n",
    "    test_idx = int(len(X) * (train_ratio +test_ratio))\n",
    "\n",
    "    # Perform the split\n",
    "    X_train, X_val, X_test = X[:train_idx], X[train_idx:test_idx], X[test_idx:]\n",
    "    y_train, y_val, y_test = y[:train_idx], y[train_idx:test_idx], y[test_idx:]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = simple_time_series_split(X, y)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors and move to GPU\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 128\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X_train.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, \n",
    "                            batch_first=True) \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 50*24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0) # Get the batch size\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device) # Initial hidden state\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device) # Initial cell state\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0)) \n",
    "        out = self.fc(out[:, -1, :]) # Get the last output of the LSTM\n",
    "        out = out.view(-1, 50, 24) # Reshape to (batch_size, mlat, mlt)\n",
    "        return out\n",
    "    \n",
    "    def reset_states(self):\n",
    "        # Reset the internal states of the LSTM layer\n",
    "        self.lstm.reset_parameters()\n",
    "\n",
    "# Instantiate the model with lookback size\n",
    "model = LSTM(8, 64, 3)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=8e-5)\n",
    "scheduler = StepLR(optimizer, step_size=8, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta(lat1, lon1, lat2, lon2):\n",
    "    radius = 6371.008\n",
    "\n",
    "    dlat = np.radians(lat2 - lat1)\n",
    "    dlon = np.radians(lon2 - lon1)\n",
    "    delta_x = dlat * radius\n",
    "    delta_y = dlon * radius * np.cos(np.radians(lat1))\n",
    "    \n",
    "    return delta_x, delta_y\n",
    "\n",
    "\n",
    "areas = np.zeros((49, 23))\n",
    "mlat = np.linspace(40, 90,50) #50 intervals between 40째 to 90째\n",
    "mlt = np.linspace(0,360,24) #24 intervals between 0째 to 360째\n",
    "\n",
    "for i in range(len(mlat) - 1):\n",
    "    for j in range(len(mlt) - 1):\n",
    "        lat1, lat2 = mlat[i], mlat[i + 1]\n",
    "        lon1, lon2 = mlt[j], mlt[j + 1]\n",
    "        \n",
    "        delta_x, delta_y = calculate_delta(lat1, lon1, lat2, lon2)\n",
    "        \n",
    "        area = delta_x * delta_y\n",
    "        areas[i, j] = area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "def weighted_loss_function(y_true, y_pred, dampener=0.1):\n",
    "    weights = areas**dampener\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    x = np.arange(weights.shape[1])\n",
    "    y = np.arange(weights.shape[0])\n",
    "    f = interpolate.interp2d(x, y, weights, kind='linear')\n",
    "    xnew = np.arange(0, weights.shape[1], weights.shape[1]/(weights.shape[1]+1))\n",
    "    ynew = np.arange(0, weights.shape[0], weights.shape[0]/(weights.shape[0]+1))\n",
    "    \n",
    "    weights = f(xnew, ynew)\n",
    "   \n",
    "    #weights = np.flipud(weights)\n",
    "    weights = np.copy(weights)\n",
    "    \n",
    "    loss = torch.mean(torch.abs(y_true - y_pred) * torch.tensor(weights, dtype=torch.float32).to(device))\n",
    "    loss = loss * 1e3\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 25\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        train_outputs = model(X_batch)\n",
    "        #print(train_outputs.shape, y_batch.shape)\n",
    "        #loss = loss_function(train_outputs, y_batch)\n",
    "        loss = weighted_loss_function(train_outputs, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            val_outputs = model(X_batch)\n",
    "            #val_loss += loss_function(val_outputs, y_batch).item()\n",
    "            val_loss += weighted_loss_function(val_outputs, y_batch).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), 'best_lstm_model.pt')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter == patience:\n",
    "            print(f'Validation loss did not improve for {patience} epochs. Stopping training.')\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss (MAE): {train_loss:.4f}, Val Loss (MAE): {val_loss:.4f}, Best Val Loss: {best_val_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.1e}')\n",
    "\n",
    "best_model = LSTM(8, 64, 3)\n",
    "best_model.load_state_dict(torch.load('best_lstm_model.pt'))\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     predictions = best_model(X_test).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calulate Global Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.mean((y_test - predictions)**2))\n",
    "NRMSE = RMSE / np.std(y_test)\n",
    "MAE = np.mean(np.abs(y_test - predictions))\n",
    "R = np.corrcoef(y_test.flatten(), predictions.flatten())[0, 1]\n",
    "\n",
    "def skill(m, o):\n",
    "    skill = 1 - (np.sum((m - o)**2) / np.sum((o - np.mean(o))**2))\n",
    "    return skill      \n",
    "\n",
    "PE = skill(predictions, y_test)\n",
    "\n",
    "print(f'Test Loss (RMSE): {RMSE:.4f}, Test Loss (NRMSE): {NRMSE:.4f}, Test Loss (MAE): {MAE:.4f}, R: {R:.4f}, Skill: {PE:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_multiple_nc(nc_files):\n",
    "    ds = xr.open_mfdataset(nc_files, concat_dim='dt', combine='nested', parallel=True)\n",
    "    ds['doy_sin'] = np.sin(2*np.pi*ds['doy']/365)\n",
    "    ds['doy_cos'] = np.cos(2*np.pi*ds['doy']/365)\n",
    "\n",
    "    #average over 5min\n",
    "    ds = ds.resample(dt='10min').mean()\n",
    "    #ds = ds.interpolate_na(dim='dt')\n",
    "\n",
    "    #ds = ds.sel(nRec=slice(16500, 17000))\n",
    "\n",
    "    return ds\n",
    "\n",
    "dir = '/home/sachin/Documents/NIPR/Research/Data/AMPERE/processed/excluded/*.nc'\n",
    "open_multi_amp = open_multiple_nc(dir)\n",
    "open_multi_amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_df(ds):\n",
    "    ds = ds.copy()\n",
    "\n",
    "    dt = ds['dt'].values.flatten()\n",
    "    BX_GSE = ds['BX_GSE'].values.flatten() #1\n",
    "    BY_GSE = ds['BY_GSE'].values.flatten() #2\n",
    "    BZ_GSE = ds['BZ_GSE'].values.flatten() #3\n",
    "    flow_speed = ds['flow_speed'].values.flatten() #4\n",
    "    proton_density = ds['proton_density'].values.flatten() #5\n",
    "    doy_sin = ds['doy_sin'].values.flatten() #6\n",
    "    doy_cos = ds['doy_cos'].values.flatten() #7\n",
    "    \n",
    "    #AL_INDEX = ds['AL_INDEX'].values.flatten() #6\n",
    "    #AU_INDEX = ds['AU_INDEX'].values.flatten() #7\n",
    "    #SYM_H = ds['SYM_H'].values.flatten() #8\n",
    "    #ASY_H = ds['ASY_H'].values.flatten()  #9\n",
    "    F10_7 = ds['F10.7'].values.flatten() #10\n",
    "    #Kp = ds['Kp'].values.flatten() #11\n",
    "    CPCP = cross_polar_cap_pot(ds['flow_speed'],  ds['BY_GSE'], ds['BZ_GSE'])\n",
    "\n",
    "    #return BX_GSE.shape\n",
    "\n",
    "    df_dict = {'dt':dt, 'BX_GSE': BX_GSE, 'BY_GSE': BY_GSE, 'BZ_GSE': BZ_GSE, 'flow_speed': flow_speed, 'proton_density': proton_density,'F10.7': F10_7, 'doy_sin': doy_sin, 'doy_cos': doy_cos}\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    df_2 = df_2.drop(columns=['dt'])\n",
    "\n",
    "    return df, df_2\n",
    "\n",
    "split_to_df(open_multi_amp)\n",
    "df_raw, df_proc = split_to_df(open_multi_amp)\n",
    "col_names = df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_arr = input_scaler.transform(df_proc)\n",
    "\n",
    "def create_sequences(arr, lookback=lookback):\n",
    "    X = []\n",
    "    for i in range(len(arr) - lookback):\n",
    "        X.append(arr[i:i+lookback].T)\n",
    "\n",
    "    X = np.array(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "proc_seq = create_sequences(norm_arr)\n",
    "raw_seq = create_sequences(df_raw.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming best_model is already defined and loaded\n",
    "device = torch.device('cpu')\n",
    "best_model.to(device)  # Ensure the model is on the CPU\n",
    "best_model.eval()\n",
    "\n",
    "# Adjust the shape to match the model's expected input size\n",
    "norm_proc = torch.tensor(proc_seq, dtype=torch.float32).to(device)\n",
    "norm_proc = norm_proc.permute(0, 1, 2)  # Change from [848370, 11, 30] to [848370, 30, 11]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(norm_proc)):\n",
    "        output = best_model(norm_proc[i].unsqueeze(0))\n",
    "        predictions.append(output)\n",
    "\n",
    "predictions = torch.cat(predictions)\n",
    "predictions = predictions.numpy()  # Tensor is already on CPU, no need to move again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset({'Jpar_pred': (['dt', 'mlat', 'mlt'], predictions),\n",
    "                 'Jpar_true': (['dt', 'mlat', 'mlt'], open_multi_amp['jPar'].values[lookback:])},\n",
    "                coords = {'dt': df_raw['dt'].values[lookback:], 'mlat': np.arange(40, 90, 1), 'mlt': np.arange(0, 24, 1)})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 83\n",
    "\n",
    "R = np.corrcoef(ds['Jpar_pred'].isel(dt=timestep).values, ds['Jpar_true'].isel(dt=timestep).values)[1,0]\n",
    "RMSE = np.sqrt(np.mean((ds['Jpar_pred'].isel(dt=timestep).values - ds['Jpar_true'].isel(dt=timestep).values)**2))\n",
    "MAE = np.mean(np.abs(ds['Jpar_pred'].isel(dt=timestep).values - ds['Jpar_true'].isel(dt=timestep).values))\n",
    "NRMSE = RMSE / (np.max(ds['Jpar_true'].isel(dt=timestep).values) - np.min(ds['Jpar_true'].isel(dt=timestep).values))\n",
    "\n",
    "print(f'R: {R:.3f}, RMSE: {RMSE:.3f}, NRMSE: {NRMSE:.3f}, MAE: {MAE:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac = ds['Jpar_pred'].isel(dt=timestep).values\n",
    "\n",
    "def dt_to_str(dt):\n",
    "    dt_1 = dt[0]\n",
    "    dt_2 = dt[1] \n",
    "    time_1 = pd.to_datetime(dt_1)\n",
    "    time_2 = pd.to_datetime(dt_2)\n",
    "    str_time_1 = time_1.strftime('%Y-%m-%d %H:%M')\n",
    "    str_time_2 = time_2.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    return str_time_1, str_time_2\n",
    "\n",
    "dt = ds['dt'].isel(dt=timestep).values\n",
    "#start_time, end_time = dt_to_str(dt)\n",
    "\n",
    "fac = fac.reshape(24, 50).T # reshape and transpose\n",
    "fac = np.flipud(fac) # flip the array upside down\n",
    "theta = np.linspace(0, 360, 24) - 90 # rotate by 90 degrees\n",
    "theta = np.radians(theta) # convert to radians\n",
    "r = 90 - np.linspace(40, 90, 50) #convert to colat\n",
    "\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"my_colormap\", [\"blue\",\"blue\", \"white\", \"white\",\"red\",\"red\"])\n",
    "#cmap = 'bwr'\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(6, 5))\n",
    "\n",
    "c = ax.contourf(theta, r, fac, cmap=cmap, levels=np.linspace(-1, 1, 100))\n",
    "ax.set_ylim([0,40])\n",
    "ax.set_yticks([0, 10, 20, 30,40])\n",
    "ax.set_yticklabels([\"90째\", \"80째\", \"70째\", \"60째\",\"50째 MLAT\"])\n",
    "ax.set_xlim([-np.pi, np.pi])\n",
    "ax.set_xticks(np.linspace(-np.pi, np.pi, 9)[1:])\n",
    "ax.set_xticklabels([\"21\", \"0 MLT \\nMidnight\", \"3\", \"6 \\n  Dawn\", \"9\", \"12 MLT \\nMidday\", \"15\", \"18 \\nDusk\"])\n",
    "ax.grid(True, linestyle='-.', alpha=0.7)\n",
    "ax.set_title(f\"{dt}\", pad=10, fontsize=11.5)\n",
    "\n",
    "\n",
    "plt.colorbar(c, ax=ax, label='J$_\\parallel$ (FAC) [쨉A/m$^2$]', shrink=0.3, pad = 0.12, \n",
    "             ticks=[-1, -0.5, 0, 0.5, 1], \n",
    "             orientation='horizontal')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac = ds['Jpar_true'].isel(dt=timestep).values\n",
    "\n",
    "def dt_to_str(dt):\n",
    "    dt_1 = dt[0]\n",
    "    dt_2 = dt[1] \n",
    "    time_1 = pd.to_datetime(dt_1)\n",
    "    time_2 = pd.to_datetime(dt_2)\n",
    "    str_time_1 = time_1.strftime('%Y-%m-%d %H:%M')\n",
    "    str_time_2 = time_2.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    return str_time_1, str_time_2\n",
    "\n",
    "dt = ds['dt'].isel(dt=timestep).values\n",
    "#start_time, end_time = dt_to_str(dt)\n",
    "\n",
    "fac = fac.reshape(24, 50).T # reshape and transpose\n",
    "fac = np.flipud(fac) # flip the array upside down\n",
    "theta = np.linspace(0, 360, 24) - 90 # rotate by 90 degrees\n",
    "theta = np.radians(theta) # convert to radians\n",
    "r = 90 - np.linspace(40, 90, 50) #convert to colat\n",
    "\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"my_colormap\", [\"blue\",\"blue\", \"white\", \"white\",\"red\",\"red\"])\n",
    "#cmap = 'bwr'\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(6, 5))\n",
    "\n",
    "c = ax.contourf(theta, r, fac, cmap=cmap, levels=np.linspace(-1, 1, 100))\n",
    "ax.set_ylim([0,40])\n",
    "ax.set_yticks([0, 10, 20, 30,40])\n",
    "ax.set_yticklabels([\"90째\", \"80째\", \"70째\", \"60째\",\"50째 MLAT\"])\n",
    "ax.set_xlim([-np.pi, np.pi])\n",
    "ax.set_xticks(np.linspace(-np.pi, np.pi, 9)[1:])\n",
    "ax.set_xticklabels([\"21\", \"0 MLT \\nMidnight\", \"3\", \"6 \\n  Dawn\", \"9\", \"12 MLT \\nMidday\", \"15\", \"18 \\nDusk\"])\n",
    "ax.grid(True, linestyle='-.', alpha=0.7)\n",
    "ax.set_title(f\"{dt}\", pad=10, fontsize=11.5)\n",
    "\n",
    "\n",
    "plt.colorbar(c, ax=ax, label='J$_\\parallel$ (FAC) [쨉A/m$^2$]', shrink=0.3, pad = 0.12, \n",
    "             ticks=[-1, -0.5, 0, 0.5,1], \n",
    "             orientation='horizontal')\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
