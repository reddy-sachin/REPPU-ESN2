{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# simple_ml_model.ipynb\n",
    "# (c) May 2024 Sachin Alexander Reddy\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input data (X) and ground truth (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Input data ---\n",
    "# This has 1 sample and 3 inputs\n",
    "# The inputs are also called features and notationally are called X (uppercase)\n",
    "X = np.array([[0.1, 0.2, 0.3]])\n",
    "\n",
    "# --- Ground truth ---\n",
    "# This has 1 sample and 1 output\n",
    "# The ground truth is also called a target and notationally is called y (lowercase)\n",
    "# The goal of a neural network is the prediction of y. This output is called y_hat, y_pred, or y prime\n",
    "y = np.array([[0.6]]) \n",
    "\n",
    "#The above is very simple and in reality you would have many samples and many features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed is fixed to make the results reproducible. This is not used in production code.\n",
    "np.random.seed(0)\n",
    "\n",
    "#--- Input and Output of model ---\n",
    "input_size = X.shape[1] # Number of features\n",
    "output_size = y.shape[1] # Number of outputs\n",
    "\n",
    "# -- Hyperparameters ---\n",
    "# Hyperparameters are parameters that are set before training\n",
    "hidden_size = 4 # Number of neurons in the hidden layer\n",
    "# Learning rate is a hyperparameter that controls how much we are adjusting the \n",
    "# weights of our network with respect the loss gradient.\n",
    "learning_rate = 0.1 \n",
    "\n",
    "#--- Weights and biases initialization ---\n",
    "#These will be updated during backpropagation.\n",
    "#Initialize weights and biases randomly. Randomness is important to break symmetry\n",
    "#between neurons and to prevent overfitting.\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1,hidden_size)) # 1 dim makes it a matrix or tensor\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1,output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass. Here the inputs are modified by the weights and biases. Then the prediction and loss are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Input layer ---\n",
    "#This is the features defined by X\n",
    "hidden_layer_input = np.dot(X, W1) + b1\n",
    "\n",
    "# --- Activation function ---\n",
    "#This is the output of the hidden layer and adds non-linearity to the model\n",
    "#Non linearity where ML gets its power\n",
    "hidden_layer_output = np.maximum(0, hidden_layer_input)\n",
    "\n",
    "# --- Hidden layers ---\n",
    "#In production code additional layers would be here\n",
    "#Multiple layers == deep learning\n",
    "#Between hidden layers 'dropout' and 'batch normalization' are often used to prevent overfitting\n",
    "\n",
    "#--- Output layer ---\n",
    "#This has same shape as y_true\n",
    "output_layer_input = np.dot(hidden_layer_output, W2) + b2 \n",
    "y_pred = output_layer_input #y_true is often denoted as $\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- 1st Prediction ---\n",
    "#By definition it is random and not accurate. No 'learning' has happened yet.\n",
    "print(f'1st Prediction (y`): {y_pred[0][0]:.2f}')\n",
    "print (f'Ground Truth (y): {y[0][0]:.2f}')\n",
    "\n",
    "#--- Loss Function ---\n",
    "#This is the difference between the predicted value and the ground truth\n",
    "#This is the error that the model will try to minimize\n",
    "#Mean Squared Error (MSE) is a common loss function\n",
    "#You will also hear this referred to as the 'cost function'. Its the same thing.\n",
    "loss_e1 = round(np.square(y_pred - y).sum(),4)\n",
    "print(f'1st MSE Loss: {loss_e1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward pass. Here the gradient of the weights are calculated using the caculus chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Backpropagation ---\n",
    "# This is the process of updating the weights and biases to minimize the loss function\n",
    "# This is where the 'learning' happens\n",
    "\n",
    "# --- Calculate Gradient of Output Layer ---\n",
    "#You can also think of this as the 'error' of the model\n",
    "#This is the difference between the predicted value and the ground truth\n",
    "grad_output = y_pred - y \n",
    "\n",
    "# --- Calculate Gradient of Hidden Layer ---\n",
    "#Caluclate gradient for Weight 2\n",
    "# We have to transpose the weights because of the dot product\n",
    "grad_W2 = np.dot(hidden_layer_output.T, grad_output) #Shape (hidden_size, output_size)\n",
    "#For now this is same value as grad_output\n",
    "grad_b2 = np.sum(grad_output, axis=0, keepdims=True) #Shape (1, output_size)\n",
    "\n",
    "#--- Calculate Gradient of Hidden Layer ---\n",
    "grad_hidden = np.dot(grad_output, W2.T) #Shape (1, hidden_size)\n",
    "grad_hidden[hidden_layer_input <= 0] = 0\n",
    "\n",
    "# --- Calculate Gradient of Input Layer ---\n",
    "grad_W1 = np.dot(X.T, grad_hidden) #Shape (input_size, hidden_size)\n",
    "grad_b1 = np.sum(grad_hidden, axis=0, keepdims=True)  #Shape (1, hidden_size)\n",
    "\n",
    "#The above shows the gradients for the weights and biases using backpropagation\n",
    "#This process uses the chain rule to calculate the gradients of the loss function with respect to the weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note the negative sign in the update step. \n",
    "#This is because we are trying to minimize the loss function\n",
    "#This is where Adam, RMSProp, and other optimization algorithms come in\n",
    "#We're using the simplest form of gradient descent here\n",
    "W1 -= learning_rate * grad_W1\n",
    "b1 -= learning_rate * grad_b1\n",
    "W2 -= learning_rate * grad_W2\n",
    "b2 -= learning_rate * grad_b2\n",
    "\n",
    "#We have now completed one iteration of the training process\n",
    "#We would repeat this process many times to train the model\n",
    "#This is called an 'epoch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 2 - Repeat the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Forward Pass --\n",
    "hidden_layer_input = np.dot(X, W1) + b1\n",
    "hidden_layer_output = np.maximum(0, hidden_layer_input)\n",
    "output_layer_input = np.dot(hidden_layer_output, W2) + b2 \n",
    "y_pred = output_layer_input #y_true is often denoted as $hat{y}$\n",
    "\n",
    "# --- Print Results ---\n",
    "loss_e2 = round(np.square(y_pred - y).sum(),4)\n",
    "print(f'2nd Prediction (y`): {y_pred[0][0]:.2f}')\n",
    "print(f'Ground Truth (y): {y[0][0]:.2f}')\n",
    "print(f'2nd MSE Loss: {loss_e2}')\n",
    "\n",
    "#--- Backpropagation ---\n",
    "grad_output = y_pred - y\n",
    "grad_W2 = np.dot(hidden_layer_output.T, grad_output)\n",
    "grad_b2 = np.sum(grad_output, axis=0, keepdims=True)\n",
    "grad_hidden = np.dot(grad_output, W2.T)\n",
    "grad_hidden[hidden_layer_input <= 0] = 0\n",
    "grad_W1 = np.dot(X.T, grad_hidden)\n",
    "grad_b1 = np.sum(grad_hidden, axis=0, keepdims=True)\n",
    "\n",
    "#--- Update Weights and Biases ---\n",
    "W1 -= learning_rate * grad_W1\n",
    "b1 -= learning_rate * grad_b1\n",
    "W2 -= learning_rate * grad_W2\n",
    "b2 -= learning_rate * grad_b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 3 - Repeat the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Forward Pass --\n",
    "hidden_layer_input = np.dot(X, W1) + b1\n",
    "hidden_layer_output = np.maximum(0, hidden_layer_input)\n",
    "output_layer_input = np.dot(hidden_layer_output, W2) + b2 \n",
    "y_pred = output_layer_input #y_true is often denoted as $hat{y}$\n",
    "\n",
    "# --- Print Results ---\n",
    "loss_e3 = round(np.square(y_pred - y).sum(),4)\n",
    "print(f'3rd Prediction (y`): {y_pred[0][0]:.2f}')\n",
    "print(f'Ground Truth (y): {y[0][0]:.2f}')\n",
    "print(f'3rd MSE Loss: {loss_e3}')\n",
    "\n",
    "#--- Backpropagation ---\n",
    "grad_output = y_pred - y\n",
    "grad_W2 = np.dot(hidden_layer_output.T, grad_output)\n",
    "grad_b2 = np.sum(grad_output, axis=0, keepdims=True)\n",
    "grad_hidden = np.dot(grad_output, W2.T)\n",
    "grad_hidden[hidden_layer_input <= 0] = 0\n",
    "grad_W1 = np.dot(X.T, grad_hidden)\n",
    "grad_b1 = np.sum(grad_hidden, axis=0, keepdims=True)\n",
    "\n",
    "#--- Update Weights and Biases ---\n",
    "W1 -= learning_rate * grad_W1\n",
    "b1 -= learning_rate * grad_b1\n",
    "W2 -= learning_rate * grad_W2\n",
    "b2 -= learning_rate * grad_b2\n",
    "\n",
    "#After 3 epochs you can see we are 'learning' and the loss function is minimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can all be combined into a single function with training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def machine_learning(X, y, learning_rate, epochs):\n",
    "    #Seed is fixed to make the results reproducible. This is not used in production code.\n",
    "    np.random.seed(0)\n",
    "\n",
    "    #--- Input and Output of model ---\n",
    "    input_size = X.shape[1] # Number of features\n",
    "    output_size = y.shape[1] # Number of output neurons\n",
    "\n",
    "    # -- Hyperparameters ---\n",
    "    hidden_size = 4 # Number of neurons in the hidden layer\n",
    "    learning_rate = 0.1 \n",
    "\n",
    "    #--- Weights and biases initialization ---\n",
    "    W1 = np.random.randn(input_size, hidden_size)\n",
    "    b1 = np.zeros((1,hidden_size)) # 1 makes nested arr aka a matrix\n",
    "    W2 = np.random.randn(hidden_size, output_size)\n",
    "    b2 = np.zeros((1,output_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Forward Pass ---\n",
    "        hidden_layer_input = np.dot(X, W1) + b1\n",
    "        hidden_layer_output = np.maximum(0, hidden_layer_input)\n",
    "        output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
    "        y_pred = output_layer_input\n",
    "\n",
    "        # --- Loss Function ---\n",
    "        loss = np.square(y_pred - y).sum()\n",
    "\n",
    "        # --- Backpropagation ---\n",
    "        grad_output = y_pred - y\n",
    "        grad_W2 = np.dot(hidden_layer_output.T, grad_output)\n",
    "        grad_b2 = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        grad_hidden = np.dot(grad_output, W2.T)\n",
    "        grad_hidden[hidden_layer_input <= 0] = 0\n",
    "        grad_W1 = np.dot(X.T, grad_hidden)\n",
    "\n",
    "        # --- Update Weights and Biases ---\n",
    "        W1 -= learning_rate * grad_W1\n",
    "        b1 -= learning_rate * grad_b1\n",
    "        W2 -= learning_rate * grad_W2\n",
    "        b2 -= learning_rate * grad_b2\n",
    "\n",
    "        print (f'Epoch {epoch+1}, y: {y[0][0]:.2f}, y`: {y_pred[0][0]:.2f}, Loss:{loss:.4f}')\n",
    "\n",
    "# --- Input data ---\n",
    "X = np.array([[0.1, 0.2, 0.3]]) # This has 1 sample, 3 features\n",
    "y = np.array([[0.6]]) # This has 1 sample, 1 output\n",
    "\n",
    "# --- Training Loop ---\n",
    "machine_learning(X, y, learning_rate=0.1, epochs=10) # adjust epochs to see how the model learns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model, load, make predictions on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---  Save Model ---\n",
    "# All this involves is saving the weights and biases\n",
    "# PyToch, TensorFlow, and other libraries do this with .save() or .state_dict() methods\n",
    "model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "np.save('model.npy', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Model ---\n",
    "model = np.load('model.npy', allow_pickle=True).item()\n",
    "W1 = model['W1']\n",
    "b1 = model['b1']\n",
    "W2 = model['W2']\n",
    "b2 = model['b2']\n",
    "\n",
    "# --- Inference ---\n",
    "#This is the process of using the trained model to make predictions\n",
    "#The reason predictions are so quick is bause we only have to do a forward pass\n",
    "#which is a simple series of dot products and activation functions\n",
    "#based on the final weights and biases\n",
    "\n",
    "hidden_layer_input = np.dot(X, W1) + b1\n",
    "hidden_layer_output = np.maximum(0, hidden_layer_input)\n",
    "output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
    "y_pred = output_layer_input\n",
    "\n",
    "print(f'Inference, y: {y[0][0]:.2f}, y`: {y_pred[0][0]:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
