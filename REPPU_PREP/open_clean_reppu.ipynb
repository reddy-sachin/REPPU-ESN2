{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m dt_str \u001b[38;5;241m=\u001b[39m folder_path[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m#get date to save the file\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# List all the files in the folder\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m file_list \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(folder_path) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, f))]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Function to interpolate and normalize the data\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minterpolate_and_normalize\u001b[39m(data, norm_factor, scale_factor):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#mon_list = ['01','02','03','04','05','06','07']\n",
    "mon_list = ['01']\n",
    "\n",
    "\n",
    "for enumerate, mon in enumerate(mon_list):\n",
    "    \n",
    "    #folder_path = f'/run/media/sachin/0fa21ddb-f70c-4238-9cf4-705e0360f1c1/NICT DUMP/idata2024/idata2024{mon}/'\n",
    "    folder_path = f'/run/media/sachin/0fa21ddb-f70c-4238-9cf4-705e0360f1c1/Test/'\n",
    "\n",
    "    # Constants\n",
    "    max_long, max_lat = 321, 221  # Maximum longitude and latitude per the raw files\n",
    "    inter_long, inter_lat, hemisphere_lat = 180, 40, 62  # Desired lon/mlt size, desired lat size, size of lat per hemisphere\n",
    "    fac_norm = 3.75e-6  # A/m^2 for FAC\n",
    "    pot_norm = 1.017e7  # V for potential\n",
    "    cond_norm = 15  # S/m for conductivity\n",
    "    chunk_size = 10000  # Number of files to process per chunk\n",
    "    #dt_str = '20200901'  # Hardcoded date extraction (as an example)\n",
    "    dt_str = folder_path[-7:-1] #get date to save the file\n",
    "\n",
    "    # List all the files in the folder\n",
    "    file_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    # Function to interpolate and normalize the data\n",
    "    def interpolate_and_normalize(data, norm_factor, scale_factor):\n",
    "        data = (data * norm_factor) * scale_factor  # Apply normalization\n",
    "        data = zoom(data, ((inter_lat/hemisphere_lat), (inter_long/max_long)), order=1)  # Interpolate over latitude and longitude\n",
    "        return data\n",
    "\n",
    "    # Function to process a chunk of files for a specific feature\n",
    "    def process_files_chunk(file_chunk, hemi, res, feature):\n",
    "        datasets = []\n",
    "        for file_name in file_chunk:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Extract datetime from filename (last 12 characters hold the datetime)\n",
    "            dt_str = file_name[-12:]\n",
    "            date_obj = pd.to_datetime(dt_str, format='%Y%m%d%H%M')\n",
    "\n",
    "            # Open and read binary file\n",
    "            with open(file_path, 'rb') as f:\n",
    "                #saving as _ overwrites each variable and thus saves memory\n",
    "                _ = np.fromfile(f, dtype=np.int32, count=1)[0]  # Count\n",
    "                _ = np.fromfile(f, dtype=np.float32, count=1)[0]  # Time\n",
    "                _ = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))  # vx\n",
    "                _ = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))  # vy\n",
    "                _ = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))  # vz\n",
    "                pot = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))\n",
    "                fac = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))\n",
    "                sxx = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))\n",
    "                syy = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))\n",
    "                sxy = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))\n",
    "\n",
    "                # Select hemisphere\n",
    "                if hemi == 'NH':\n",
    "                    s_ind, e_ind = 158, 220\n",
    "                else:\n",
    "                    s_ind, e_ind = 0, hemisphere_lat\n",
    "\n",
    "                # Extract the feature data\n",
    "                feature_data = {\n",
    "                    'pot': pot[s_ind:e_ind, :],\n",
    "                    'fac': fac[s_ind:e_ind, :],\n",
    "                    'sxx': sxx[s_ind:e_ind, :],\n",
    "                    'syy': syy[s_ind:e_ind, :],\n",
    "                    'sxy': sxy[s_ind:e_ind, :]\n",
    "                }[feature] # Select the feature\n",
    "\n",
    "                # Apply normalization and interpolation based on the feature\n",
    "                if feature == 'pot':\n",
    "                    feature_data = interpolate_and_normalize(feature_data, pot_norm, scale_factor=1e-3)  # Convert V to kV\n",
    "                elif feature == 'fac':\n",
    "                    feature_data = interpolate_and_normalize(feature_data, fac_norm, scale_factor=1e6) # Convert A/m^2 to uA/m^2\n",
    "                else:\n",
    "                    feature_data = interpolate_and_normalize(feature_data, cond_norm, scale_factor=1) # No conversion for conductivity\n",
    "\n",
    "                # Create xarray Dataset\n",
    "                ds = xr.Dataset(\n",
    "                    {\n",
    "                        feature: (['lat', 'lon'], feature_data)\n",
    "                    },\n",
    "                    coords={\n",
    "                        'dt': date_obj,\n",
    "                        'lat': np.linspace(50, 90, inter_lat),\n",
    "                        'lon': np.arange(1, 361, 2),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Append to list\n",
    "                datasets.append(ds)\n",
    "\n",
    "        # Concatenate all datasets in the list\n",
    "        ds_full = xr.concat(datasets, dim='dt')\n",
    "        ds_full = ds_full.resample(dt=f'{res}min').mean()\n",
    "        ds_full = ds_full.interpolate_na(dim='dt')\n",
    "\n",
    "        return ds_full\n",
    "\n",
    "    # PARAMETERS\n",
    "    feature_list = ['pot', 'fac', 'sxx', 'syy', 'sxy']\n",
    "    #feature_list =  ['fac']\n",
    "    hemi = 'NH'  # Select hemisphere\n",
    "    resolution = 2  # Set resolution in minutes\n",
    "\n",
    "    combined_features = []  # Initialize list to store combined datasets for each feature\n",
    "    for feature in feature_list:\n",
    "        feature_ds = None  # Initialize combined dataset for each feature\n",
    "\n",
    "        for i in range(0, len(file_list), chunk_size):\n",
    "\n",
    "            file_chunk = file_list[i:i+chunk_size] # Select a chunk of files\n",
    "            chunk_ds = process_files_chunk(file_chunk, hemi, resolution, feature) # Process the chunk\n",
    "\n",
    "            # Concatenate datasets across chunks\n",
    "            if feature_ds is None:\n",
    "                feature_ds = chunk_ds\n",
    "            else:\n",
    "                feature_ds = xr.concat([feature_ds, chunk_ds], dim='dt')\n",
    "\n",
    "        # Save the dataset to a NetCDF file\n",
    "        #feature_ds.to_netcdf(f'/run/media/sachin/0fa21ddb-f70c-4238-9cf4-705e0360f1c1/NICT_Data/test/{dt_str}_{feature}_{inter_long}_{inter_lat}_{resolution}min.nc')\n",
    "\n",
    "        # Append to the list\n",
    "        combined_features.append(feature_ds)\n",
    "\n",
    "    # Combine all features into a single dataset (for testing below, not for ML)\n",
    "    combined_ds = xr.merge(combined_features)\n",
    "    combined_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(7, 6))\n",
    "\n",
    "for i, var in enumerate(['pot', 'fac', 'sxx', 'syy', 'sxy']):\n",
    "    combined_ds[var].plot(ax=ax[i//2, i%2])\n",
    "    ax[i//2, i%2].set_title(var)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common parameters\n",
    "theta = np.deg2rad(np.linspace(0, 360, inter_long) - 90)\n",
    "r = 90 - np.linspace(53.1, 89.7, inter_lat)\n",
    "shrink = 0.4\n",
    "fig, ax = plt.subplots(2, 2, subplot_kw={'projection': 'polar'}, figsize=(10, 10))\n",
    "\n",
    "#potential\n",
    "mesh1 = ax[0, 0].pcolormesh(theta, r, combined_ds['pot'].mean('dt'), shading='auto', cmap='bwr', vmin=-20, vmax=20)\n",
    "ax[0, 0].set_title('Potential')\n",
    "fig.colorbar(mesh1, ax=ax[0, 0], label='kV', orientation='horizontal', pad=0.1, shrink=shrink, extend='both')\n",
    "\n",
    "#current\n",
    "mesh2 = ax[0, 1].pcolormesh(theta, r, combined_ds['fac'].mean('dt'), shading='auto', cmap='bwr', vmin = -1, vmax = 1)\n",
    "ax[0, 1].set_title('Current')\n",
    "fig.colorbar(mesh2, ax=ax[0, 1], label=r'$\\mathrm{\\mu}$A/m$^2$', orientation='horizontal', pad=0.1, shrink=shrink, extend='both')\n",
    "\n",
    "#sxx\n",
    "mesh3 = ax[1, 0].pcolormesh(theta, r, combined_ds['sxx'].mean('dt'), shading='auto', cmap='viridis', vmin=0, vmax=15)\n",
    "ax[1, 0].set_title('Conductivity (xx)')\n",
    "fig.colorbar(mesh3, ax=ax[1, 0], label='S/m', orientation='horizontal', pad=0.1, shrink=shrink, extend='both')\n",
    "\n",
    "#syy\n",
    "if hemi == 'NH':\n",
    "    mesh4 = ax[1, 1].pcolormesh(theta, r, combined_ds['syy'].mean('dt'), shading='auto', cmap='viridis', vmin=0, vmax=15)\n",
    "else:\n",
    "    mesh4 = ax[1, 1].pcolormesh(theta, r, combined_ds['syy'].mean('dt'), shading='auto', cmap='viridis_r', vmin=-15, vmax=0)\n",
    "ax[1, 1].set_title('Conductivity (yy)')\n",
    "fig.colorbar(mesh4, ax=ax[1, 1], label='S/m', orientation='horizontal', pad=0.1, shrink=shrink, extend='both')\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax[i,j].set_ylim([0, 37])\n",
    "        ax[i,j].set_yticks([0, 10, 20, 30])\n",
    "        ax[i,j].set_yticklabels([\"90째\", \"80째\", \"70째\", \"60째 MLAT\"])\n",
    "        ax[i,j].set_xlim([-np.pi, np.pi])\n",
    "        ax[i,j].set_xticks(np.linspace(-np.pi, np.pi, 9)[1:])\n",
    "        ax[i,j].set_xticklabels([\"21\", \"0 MLT \\nMidnight\", \"3\", \"6 \\n  Dawn\", \"9\", \"12 MLT \\nMidday\", \"15\", \"18 \\nDusk\"])\n",
    "        ax[i,j].grid(True, linestyle='-', linewidth=0.5, zorder=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "#space between subplots\n",
    "plt.subplots_adjust(wspace=-0.2, hspace=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
