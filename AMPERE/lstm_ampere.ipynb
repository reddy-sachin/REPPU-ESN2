{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/home/sachin/Documents/NIPR/Research/Data/AMPERE/processed/ampere_omni_2010_2013.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpar = ds['jPar'].values.flatten()\n",
    "jpar = jpar[::1000]\n",
    "sns.histplot(jpar, bins=100, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers for the target variable and input variables\n",
    "input_scaler = MinMaxScaler()\n",
    "\n",
    "# Extract the target variable and reshape for scaling\n",
    "target_var = ds['jPar'].values  # shape (t (n), mlat (50), mlt (24))\n",
    "\n",
    "# Extract and scale input variables (variables that are dependent only on 'dt')\n",
    "input_vars = ['BX_GSE', 'BY_GSE', 'BZ_GSE', 'flow_speed', 'proton_density', 'AL_INDEX', 'AU_INDEX', 'SYM_H', 'ASY_H', 'F10.7', 'Kp']\n",
    "input_data = np.array([ds[var].values for var in input_vars]).T  # shape (22320, number_of_vars)\n",
    "input_data_scaled = input_scaler.fit_transform(input_data)\n",
    "\n",
    "def create_sequences(target_data, input_data, lookback=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(target_data) - lookback):\n",
    "        X.append(input_data[i:i+lookback].T)\n",
    "        y.append(target_data[i+lookback])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "lookback = 30\n",
    "X, y = create_sequences(target_var, input_data_scaled, lookback=lookback)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Calculate split indices\n",
    "train_idx = int(len(X) * train_ratio)\n",
    "val_idx = int(len(X) * (train_ratio + val_ratio))\n",
    "\n",
    "# Perform the split\n",
    "X_train, X_val, X_test = X[:train_idx], X[train_idx:val_idx], X[val_idx:]\n",
    "y_train, y_val, y_test = y[:train_idx], y[train_idx:val_idx], y[val_idx:]\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_time_series_split(X, y, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, segment_length=720):\n",
    "    total_segments = len(X) // segment_length\n",
    "    total_samples = total_segments * segment_length\n",
    "    \n",
    "    # Shuffle indices to randomly select segments\n",
    "    indices = np.arange(total_segments)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Calculate sizes of train, validation, and test sets\n",
    "    train_size = int(total_segments * train_ratio)\n",
    "    val_size = int(total_segments * val_ratio)\n",
    "    test_size = total_segments - train_size - val_size\n",
    "    \n",
    "    # Select indices for train, validation, and test sets\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "    \n",
    "    # Function to extract segments\n",
    "    def extract_segments(indices):\n",
    "        X_segments = []\n",
    "        y_segments = []\n",
    "        for idx in indices:\n",
    "            start_idx = idx * segment_length\n",
    "            X_segment = X[start_idx:start_idx + segment_length]\n",
    "            y_segment = y[start_idx:start_idx + segment_length]\n",
    "            X_segments.append(X_segment)\n",
    "            y_segments.append(y_segment)\n",
    "        return np.concatenate(X_segments), np.concatenate(y_segments)\n",
    "    \n",
    "    # Extract train, validation, and test sets\n",
    "    X_train, y_train = extract_segments(train_indices)\n",
    "    X_val, y_val = extract_segments(val_indices)\n",
    "    X_test, y_test = extract_segments(test_indices)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = random_time_series_split(X, y)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert data to PyTorch tensors and move to device\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=lookback, hidden_size=64, num_layers=3, batch_first=True)\n",
    "        self.fc = nn.Linear(64, 50*24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        x = x.view(-1, 50, 24)\n",
    "        return x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 40*24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = out.view(-1, 40, 24)\n",
    "        return out\n",
    "    \n",
    "    def reset_states(self):\n",
    "        # Reset the internal states of the LSTM layer\n",
    "        self.lstm.reset_parameters()\n",
    "\n",
    "# Instantiate the model with the correct input size\n",
    "model = LSTM(lookback, 64, 2)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "#early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        train_outputs = model(X_batch)\n",
    "        loss = loss_function(train_outputs, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            val_outputs = model(X_batch)\n",
    "            val_loss += loss_function(val_outputs, y_batch).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    #Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f'Counter is at {counter}')\n",
    "        \n",
    "        if counter == patience:\n",
    "            print(f'Validation loss did not improve for {patience} epochs. Stopping training.')\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        #batch_X = batch_X.permute(0, 2, 1)\n",
    "\n",
    "        output = model(batch_X)\n",
    "        loss = loss_function(output, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        predictions.append(output)\n",
    "\n",
    "predictions = torch.cat(predictions)\n",
    "predictions = predictions.cpu().numpy()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "microamp = r'$\\mu$A'\n",
    "print(f'Test Loss: {np.sqrt(avg_test_loss):.3f} {microamp}')\n",
    "\n",
    "mlat_idx = 39  #0 through 49\n",
    "mlt_idx = 1 #0 through 23\n",
    "\n",
    "#y_test = y_test.cpu().numpy()\n",
    "\n",
    "predictions_mean = np.mean(predictions, axis=0)\n",
    "y_test_mean = np.mean(y_test, axis=0)\n",
    "\n",
    "R = np.corrcoef(y_test_mean[:, mlat_idx, mlt_idx], predictions_mean[:, mlat_idx, mlt_idx])[0,1]\n",
    "\n",
    "R = np.corrcoef(y_test[:, mlat_idx, mlt_idx], predictions[:, mlat_idx, mlt_idx])[0,1]\n",
    "MAE = np.mean(np.abs(y_test[:, mlat_idx, mlt_idx] - predictions[:, mlat_idx, mlt_idx]))\n",
    "RMSE = np.sqrt(np.mean((y_test[:, mlat_idx, mlt_idx] - predictions[:, mlat_idx, mlt_idx])**2))\n",
    "NRMSE = RMSE / (np.max(y_test[:, mlat_idx, mlt_idx]) - np.min(y_test[:, mlat_idx, mlt_idx]))\n",
    "print(f'R: {R:.3f}, RMSE: {RMSE:.3f}, NRMSE: {NRMSE:.3f}, MAE: {MAE:.3f}')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(y_test[:, mlat_idx, mlt_idx], label='Actual')\n",
    "plt.plot(predictions[:, mlat_idx, mlt_idx], label='Predicted')\n",
    "plt.ylim(-1,1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mean = np.mean(predictions, axis=0)\n",
    "y_test_mean = np.mean(y_test, axis=0)\n",
    "predictions_mean.shape, y_test_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac = predictions_mean\n",
    "\n",
    "def dt_to_str(dt):\n",
    "    dt_1 = dt[0]\n",
    "    dt_2 = dt[1] \n",
    "    time_1 = pd.to_datetime(dt_1)\n",
    "    time_2 = pd.to_datetime(dt_2)\n",
    "    str_time_1 = time_1.strftime('%Y-%m-%d %H:%M')\n",
    "    str_time_2 = time_2.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    return str_time_1, str_time_2\n",
    "\n",
    "#dt = open_amp['dt'].values\n",
    "#start_time, end_time = dt_to_str(dt)\n",
    "\n",
    "fac = fac.reshape(y_test_mean.shape[1], y_test_mean.shape[0]).T # reshape and transpose\n",
    "fac = np.flipud(fac) # flip the array upside down\n",
    "theta = np.linspace(0, 360, y_test_mean.shape[1]) - 90 # rotate by 90 degrees\n",
    "theta = np.radians(theta) # convert to radians\n",
    "r = 90 - np.linspace(50, 90, y_test_mean.shape[0]) #convert to colat\n",
    "\n",
    "#cmap = mcolors.LinearSegmentedColormap.from_list(\"my_colormap\", [\"blue\",\"blue\", \"white\", \"white\",\"red\",\"red\"])\n",
    "cmap = 'bwr'\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(6, 5))\n",
    "\n",
    "c = ax.contourf(theta, r, fac, cmap=cmap, levels=np.linspace(-0.5,.5 , 100))\n",
    "ax.set_ylim([0,40])\n",
    "ax.set_yticks([0, 10, 20, 30,40])\n",
    "ax.set_yticklabels([\"90°\", \"80°\", \"70°\", \"60°\",\"50° MLAT\"])\n",
    "ax.set_xlim([-np.pi, np.pi])\n",
    "ax.set_xticks(np.linspace(-np.pi, np.pi, 9)[1:])\n",
    "ax.set_xticklabels([\"21\", \"0 MLT \\nMidnight\", \"3\", \"6 \\n  Dawn\", \"9\", \"12 MLT \\nMidday\", \"15\", \"18 \\nDusk\"])\n",
    "ax.grid(True, linestyle='-.', alpha=0.7)\n",
    "#ax.set_title(f\"{start_time} - {end_time}\", pad=10, fontsize=11.5)\n",
    "\n",
    "\n",
    "plt.colorbar(c, ax=ax, label='J$_\\parallel$ (FAC) [µA/m$^2$]', shrink=0.3, pad = 0.12, \n",
    "             ticks=[-0.5, 0,  0.5], \n",
    "             orientation='horizontal')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(fac, cmap=cmap, levels=np.linspace(-0.5, 0.5, 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
