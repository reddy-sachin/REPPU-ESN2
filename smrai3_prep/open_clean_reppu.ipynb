{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.interpolate import interp1d\n",
    "import datetime as datetime \n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interpolate and normalize the data\n",
    "def interpolate_and_normalize(data, norm_factor, scale_factor):\n",
    "    data = (data * norm_factor) * scale_factor  # Apply normalization\n",
    "    data = zoom(data, ((inter_lat/hemisphere_lat), (inter_long/max_long)), order=1)  # Interpolate over latitude and longitude\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_35763/2642631865.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  lat_df = pd.read_csv('latitude_table.txt', sep='\\s+', header=0)\n"
     ]
    }
   ],
   "source": [
    "def get_latitude_indices(lat_ind, lat_size):\n",
    "    \n",
    "    #Open the latitude table and clean it up\n",
    "    lat_df = pd.read_csv('latitude_table.txt', sep='\\s+', header=0)\n",
    "    lat_df = lat_df.dropna(axis=1)\n",
    "    lat_df = lat_df.drop(lat_df.columns[0], axis=1)\n",
    "    lat_df.columns = ['rad', 'colat']\n",
    "    lat_df['lat'] = lat_df['colat'] - 90 # Convert colatitude to latitude\n",
    "\n",
    "    # Interpolate the latitude values to match the desired resolution\n",
    "    # whilst ensuring the spacing is non-uniform per the REPPU grid\n",
    "    latitudes = lat_df['lat'].iloc[0:lat_ind].values # Get the first lat_ind values\n",
    "    indices = np.linspace(0, len(latitudes) - 1, lat_size) # Create the indices for the interpolation\n",
    "    interp_func = interp1d(np.arange(len(latitudes)), latitudes, kind='linear') # Create the interpolation function\n",
    "    interpolated_latitudes = np.flip(interp_func(indices)) # Interpolate the latitudes and flip them to be in ascending order\n",
    "\n",
    "    return interpolated_latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing feature: Potential [kV]\n",
      "Processed: 2020-08, Time taken: 0:13:49.314533, Memory used: 2.30 GB\n",
      "Processed: 2020-09, Time taken: 0:10:26.033858, Memory used: 2.28 GB\n",
      "Processed: 2020-10, Time taken: 0:13:03.200045, Memory used: 2.38 GB\n",
      "Processed: 2020-11, Time taken: 0:14:17.725023, Memory used: 2.28 GB\n",
      "Processed: 2020-12, Time taken: 0:13:21.852195, Memory used: 2.38 GB\n",
      "Processed: 2021-01, Time taken: 0:13:06.809613, Memory used: 2.38 GB\n",
      "Processed: 2021-02, Time taken: 0:11:56.845611, Memory used: 2.26 GB\n",
      "Processed: 2021-03, Time taken: 0:13:34.153517, Memory used: 2.38 GB\n",
      "Processed: 2021-04, Time taken: 0:13:25.453709, Memory used: 2.36 GB\n",
      "Processed: 2021-05, Time taken: 0:14:08.384038, Memory used: 2.39 GB\n",
      "Processed: 2021-06, Time taken: 0:14:38.771960, Memory used: 2.37 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m _ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromfile(f, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Count\u001b[39;00m\n\u001b[1;32m     62\u001b[0m _ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromfile(f, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Time\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_long\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmax_lat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape((max_lat, max_long))  \u001b[38;5;66;03m# vx\u001b[39;00m\n\u001b[1;32m     64\u001b[0m _ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromfile(f, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, count\u001b[38;5;241m=\u001b[39mmax_long\u001b[38;5;241m*\u001b[39mmax_lat)\u001b[38;5;241m.\u001b[39mreshape((max_lat, max_long))  \u001b[38;5;66;03m# vy\u001b[39;00m\n\u001b[1;32m     65\u001b[0m _ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromfile(f, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, count\u001b[38;5;241m=\u001b[39mmax_long\u001b[38;5;241m*\u001b[39mmax_lat)\u001b[38;5;241m.\u001b[39mreshape((max_lat, max_long))  \u001b[38;5;66;03m# vz\u001b[39;00m\n",
      "File \u001b[0;32m<frozen codecs>:335\u001b[0m, in \u001b[0;36msetstate\u001b[0;34m(self, state)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# PARAMETERS (Change as needed)\n",
    "folder_path_template = '/run/media/sachin/0fa21ddb-f70c-4238-9cf4-705e0360f1c1/NICT DUMP/idata{}/idata{}/'\n",
    "inter_lat, inter_long = 40, 140 #Desired lon/mlt size in deg, desired lat size in deg #140 = 10 mins of MLT\n",
    "hemisphere_lat = 63  #Desired lat size in index (check latitude_table.txt)\n",
    "hemi = 'SH'  # Hemisphere to run: 'NH' or 'SH'\n",
    "resolution = '2' #set to 2 mins, options: 1, 2, 5\n",
    "\n",
    "# CONSTANTS (Change with caution)\n",
    "max_long, max_lat = 321, 221  # Maximum longitude and latitude per the raw files\n",
    "latitudes = get_latitude_indices(hemisphere_lat, inter_lat) #intepolate latitude maintainig correct spacing per latitude_table.txt\n",
    "fac_norm = 3.75e-6  # A/m^2 for FAC\n",
    "pot_norm = 1.017e7  # V for potential\n",
    "cond_norm = 15  # S/m for conductivity\n",
    "\n",
    "# Create a dictionary to map the features to the respective names\n",
    "#and their respective units\n",
    "feature_map = {\n",
    "    'pot': 'Potential [kV]',\n",
    "    'fac': 'Field-aligned current [uA/m^2]',\n",
    "    'sxx': 'Conductivity (xx) [S/m]',\n",
    "    'syy': 'Conductivity (yy) [S/m]',\n",
    "    'sxy': 'Conductivity (xy) [S/m]'\n",
    "}\n",
    "\n",
    "# Create a dictionary to map the years to the respective months\n",
    "# If we get more data in the future, we can add more months to the respective years\n",
    "\n",
    "year_month_map = {\n",
    "    '2020': ['08', '09', '10', '11', '12'],\n",
    "    '2021': ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'],\n",
    "    '2022': ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '12'],\n",
    "    '2023': ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'],\n",
    "    '2024': ['01', '02', '03', '04', '05', '06', '07']\n",
    "}\n",
    "\n",
    "\n",
    "# Loop through the years and respective months\n",
    "for feature in feature_map.keys(): #take keys only \n",
    "    print(f'Processing feature: {feature_map[feature]}')\n",
    "    for year, months in year_month_map.items():\n",
    "        for month in months:\n",
    "\n",
    "            folder_path = folder_path_template.format(year, year + month)\n",
    "\n",
    "            date_str = folder_path[-7:-1] #extract date for saving file name\n",
    "            feature_list = []  \n",
    "            dt_list = []\n",
    "\n",
    "            start = datetime.datetime.now() #track time (for testing)\n",
    "            process = psutil.Process(os.getpid())  # Get current process for memory tracking (for testing)\n",
    "\n",
    "            for file_name in os.listdir(folder_path):\n",
    "\n",
    "                # Extract datetime from filename (last 12 characters hold the datetime)\n",
    "                dt_str = file_name[-12:]\n",
    "                date_obj = pd.to_datetime(dt_str, format='%Y%m%d%H%M')\n",
    "\n",
    "                with open(os.path.join(folder_path, file_name), 'r') as f:\n",
    "\n",
    "                    #We are not interested in the first 5 values, so we overwrite them to save memory\n",
    "                    _ = np.fromfile(f, dtype=np.int32, count=1)[0]  # Count\n",
    "                    _ = np.fromfile(f, dtype=np.float32, count=1)[0]  # Time\n",
    "                    _ = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))  # vx\n",
    "                    _ = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))  # vy\n",
    "                    _ = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))  # vz\n",
    "\n",
    "                    #potential [V], field aligned current [uA/m^2], conductivity (xx, yy, xy) [S/m]\n",
    "                    pot = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))\n",
    "                    fac = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))\n",
    "                    sxx = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))\n",
    "                    syy = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))\n",
    "                    sxy = np.fromfile(f, dtype=np.float32, count=max_long*max_lat).reshape((max_lat, max_long))\n",
    "\n",
    "                    # Select hemisphere\n",
    "                    if hemi == 'NH':\n",
    "                        s_ind, e_ind = (max_lat - 1) - hemisphere_lat, max_lat - 1\n",
    "                    else:\n",
    "                        s_ind, e_ind = 0, hemisphere_lat\n",
    "\n",
    "                    # Extract the feature data\n",
    "                    feature_data = {\n",
    "                        'pot': pot[s_ind:e_ind, :],\n",
    "                        'fac': fac[s_ind:e_ind, :],\n",
    "                        'sxx': sxx[s_ind:e_ind, :],\n",
    "                        'syy': syy[s_ind:e_ind, :],\n",
    "                        'sxy': sxy[s_ind:e_ind, :]\n",
    "                    }[feature] # Select the feature\n",
    "\n",
    "                    # Apply normalization and interpolation based on the feature\n",
    "                    if feature == 'pot':\n",
    "                        feature_data = interpolate_and_normalize(feature_data, pot_norm, scale_factor=1e-3)  # Convert V to kV\n",
    "                    elif feature == 'fac':\n",
    "                        feature_data = interpolate_and_normalize(feature_data, fac_norm, scale_factor=1e6) # Convert A/m^2 to uA/m^2\n",
    "                    else:\n",
    "                        feature_data = interpolate_and_normalize(feature_data, cond_norm, scale_factor=1) # No conversion for conductivity\n",
    "\n",
    "                    #Append feature data and date to lists\n",
    "                    feature_list.append(feature_data)\n",
    "                    dt_list.append(date_obj)\n",
    "\n",
    "                    #for testing\n",
    "                    #set to between 1(1 file) and 1000(1000 files)\n",
    "                    #if len(feature_list) == 1440:\n",
    "                    #    break\n",
    "                    \n",
    "            # Create xarray dataset\n",
    "            ds = xr.Dataset(\n",
    "                {\n",
    "                    feature: (['dt', 'lat', 'lon'], feature_list),  # Ensure feature_list is in shape (time, lat, lon)\n",
    "                },\n",
    "                coords={\n",
    "                    'dt': dt_list,  # Datetime list\n",
    "                    'lat': latitudes,  # Latitude indices or values\n",
    "                    'lon': np.linspace(0, 360, inter_long) # Longitude values\n",
    "                }\n",
    "            )\n",
    "\n",
    "            ds = ds.resample(dt=f'{resolution}min').mean()  # Resample to n minute intervals\n",
    "            ds = ds.interpolate_na(dim='dt')  # Interpolate any missing values\n",
    "\n",
    "            # Track time and memory\n",
    "            end = datetime.datetime.now()\n",
    "            total_time = end - start\n",
    "            final_mem_usage = process.memory_info().rss / (1024 ** 3)  # Convert to GB\n",
    "            print(f'Processed: {year}-{month}, Time taken: {total_time}, Memory used: {final_mem_usage:.2f} GB')\n",
    "\n",
    "            # Save the dataset\n",
    "            ds.to_netcdf(f'/run/media/sachin/0fa21ddb-f70c-4238-9cf4-705e0360f1c1/NICT_Data/{hemi}/{year}/{date_str}_{feature}_{inter_long}_{inter_lat}_2min.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common parameters\n",
    "theta = np.deg2rad(np.linspace(0, 360, inter_long) - 90)\n",
    "r = 90 - np.linspace(53.1, 89.7, inter_lat)\n",
    "shrink = 0.4\n",
    "fig, ax = plt.subplots(1, 1, subplot_kw={'projection': 'polar'}, figsize=(5, 5))\n",
    "\n",
    "\n",
    "if feature == 'fac':\n",
    "    #current\n",
    "    mesh2 = ax.pcolormesh(theta, r, ds[feature].mean('dt'), shading='auto', cmap='bwr', vmin = -1, vmax = 1)\n",
    "    ax.set_title('Current')\n",
    "    fig.colorbar(mesh2, ax=ax, label=r'$\\mathrm{\\mu}$A/m$^2$', orientation='horizontal', pad=0.15, shrink=shrink, extend='both')\n",
    "elif feature == 'pot':\n",
    "    #potential\n",
    "    mesh1 = ax.pcolormesh(theta, r, ds[feature].mean('dt'), shading='auto', cmap='bwr', vmin=-20, vmax=20)\n",
    "    ax.set_title('Potential')\n",
    "    fig.colorbar(mesh1, ax=ax, label='kV', orientation='horizontal', pad=0.15, shrink=shrink, extend='both')\n",
    "elif feature == 'sxx':\n",
    "    #sxx\n",
    "    mesh3 = ax.pcolormesh(theta, r, ds[feature].mean('dt'), shading='auto', cmap='viridis', vmin=0, vmax=15)\n",
    "    ax.set_title('Conductivity (xx)')\n",
    "    fig.colorbar(mesh3, ax=ax, label='S/m', orientation='horizontal', pad=0.15, shrink=shrink, extend='both')\n",
    "\n",
    "ax.set_ylim([0, 37])\n",
    "ax.set_yticks([0, 10, 20, 30])\n",
    "ax.set_yticklabels([\"90째\", \"80째\", \"70째\", \"60째 MLAT\"])\n",
    "ax.set_xlim([-np.pi, np.pi])\n",
    "ax.set_xticks(np.linspace(-np.pi, np.pi, 9)[1:])\n",
    "ax.set_xticklabels([\"21\", \"0 MLT \\nMidnight\", \"3\", \"6 \\n  Dawn\", \"9\", \"12 MLT \\nMidday\", \"15\", \"18 \\nDusk\"])\n",
    "ax.grid(True, linestyle='-', linewidth=0.5, zorder=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "#space between subplots\n",
    "#plt.savefig('lon_180.png', dpi=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
